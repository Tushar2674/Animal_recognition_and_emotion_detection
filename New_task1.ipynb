{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a08646b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout ,Input \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810a8417",
   "metadata": {},
   "source": [
    "# Animal recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3c2b2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'animals/train'\n",
    "validation_dir = 'animals/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abb024b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1152 images belonging to 24 classes.\n",
      "Found 80 images belonging to 24 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Apply data augmentation to the training data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2  # Split for validation\n",
    ")\n",
    "\n",
    "# Validation data should not be augmented\n",
    "val_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "# Load and preprocess the datasets\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52dde0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 55m 14s]\n",
      "val_accuracy: 0.9312500059604645\n",
      "\n",
      "Best val_accuracy So Far: 0.9312500059604645\n",
      "Total elapsed time: 03h 19m 45s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\saving\\saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 118 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 815ms/step - accuracy: 0.9913 - loss: 0.1846 - val_accuracy: 0.9500 - val_loss: 0.3196\n",
      "Epoch 2/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 754ms/step - accuracy: 0.9960 - loss: 0.0366 - val_accuracy: 0.9250 - val_loss: 0.3487\n",
      "Epoch 3/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 778ms/step - accuracy: 0.9959 - loss: 0.0322 - val_accuracy: 0.9375 - val_loss: 0.3600\n",
      "Epoch 4/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 747ms/step - accuracy: 0.9958 - loss: 0.0216 - val_accuracy: 0.9125 - val_loss: 0.3383\n",
      "Epoch 5/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 764ms/step - accuracy: 0.9917 - loss: 0.0299 - val_accuracy: 0.9125 - val_loss: 0.3964\n",
      "Epoch 6/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 748ms/step - accuracy: 0.9931 - loss: 0.0243 - val_accuracy: 0.9125 - val_loss: 0.3807\n",
      "Epoch 7/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 758ms/step - accuracy: 0.9967 - loss: 0.0130 - val_accuracy: 0.9000 - val_loss: 0.4027\n",
      "Epoch 8/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 785ms/step - accuracy: 0.9993 - loss: 0.0096 - val_accuracy: 0.9125 - val_loss: 0.3866\n",
      "Epoch 9/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 787ms/step - accuracy: 0.9994 - loss: 0.0046 - val_accuracy: 0.9125 - val_loss: 0.4110\n",
      "Epoch 10/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 772ms/step - accuracy: 0.9991 - loss: 0.0097 - val_accuracy: 0.9000 - val_loss: 0.4429\n",
      "Found 432 images belonging to 24 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 617ms/step - accuracy: 0.9654 - loss: 0.1098\n",
      "Best Animal Recognition Model Test Accuracy: 0.97\n",
      "Best Animal Recognition Model Test Loss: 0.14\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from keras_tuner import RandomSearch\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "import os\n",
    "# Verify number of classes\n",
    "num_classes = train_generator.num_classes\n",
    "print(f'Number of classes: {num_classes}')\n",
    "\n",
    "# Pre-trained model\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "# Fine-tune the model\n",
    "base_model.trainable = True\n",
    "fine_tune_at = 100\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Function to build the model (for hyperparameter tuning)\n",
    "def build_model(hp):\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(hp.Int('units', min_value=128, max_value=512, step=64), activation='relu'),\n",
    "        Dropout(hp.Float('dropout', min_value=0.2, max_value=0.5, step=0.1)),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Hyperparameter tuning\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=2,\n",
    "    directory='my_dir',\n",
    "    project_name='animal_recognition'\n",
    ")\n",
    "\n",
    "# Perform the search\n",
    "tuner.search(train_generator, validation_data=validation_generator, epochs=10)\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Further train the best model\n",
    "best_model.fit(train_generator, validation_data=validation_generator, epochs=10)\n",
    "\n",
    "# Evaluate the best model\n",
    "animal_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "animal_test_generator = animal_test_datagen.flow_from_directory(validation_dir, target_size=(128, 128), batch_size=32, class_mode='categorical')\n",
    "\n",
    "test_loss, test_accuracy = best_model.evaluate(animal_test_generator)\n",
    "print(f'Best Animal Recognition Model Test Accuracy: {test_accuracy:.2f}')\n",
    "print(f'Best Animal Recognition Model Test Loss: {test_loss:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec38569",
   "metadata": {},
   "source": [
    "# Animal Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c6b194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_train_dir = 'Master Folder/train'\n",
    "emotion_validation_dir = 'Master Folder/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47d92e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 images belonging to 4 classes.\n",
      "Found 7 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Apply data augmentation to the training data\n",
    "emotion_train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2  # Split for validation\n",
    ")\n",
    "\n",
    "# Validation data should not be augmented\n",
    "emotion_val_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "# Load and preprocess the datasets\n",
    "emotion_train_generator = emotion_train_datagen.flow_from_directory(\n",
    "    emotion_train_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "emotion_validation_generator = emotion_val_datagen.flow_from_directory(\n",
    "    emotion_validation_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44fd06e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 Complete [00h 09m 23s]\n",
      "val_accuracy: 0.5000000149011612\n",
      "\n",
      "Best val_accuracy So Far: 0.5000000149011612\n",
      "Total elapsed time: 02h 45m 35s\n",
      "Epoch 1/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 861ms/step - accuracy: 0.4228 - loss: 1.3189 - val_accuracy: 0.5714 - val_loss: 1.4525\n",
      "Epoch 2/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 712ms/step - accuracy: 0.4773 - loss: 1.2333 - val_accuracy: 0.5714 - val_loss: 1.4594\n",
      "Epoch 3/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 698ms/step - accuracy: 0.4791 - loss: 1.2084 - val_accuracy: 0.5714 - val_loss: 1.4460\n",
      "Epoch 4/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 672ms/step - accuracy: 0.4702 - loss: 1.1839 - val_accuracy: 0.5714 - val_loss: 1.4312\n",
      "Epoch 5/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 631ms/step - accuracy: 0.5384 - loss: 1.1265 - val_accuracy: 0.5714 - val_loss: 1.4275\n",
      "Epoch 6/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 612ms/step - accuracy: 0.4995 - loss: 1.1359 - val_accuracy: 0.5714 - val_loss: 1.4301\n",
      "Epoch 7/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 598ms/step - accuracy: 0.5601 - loss: 1.0621 - val_accuracy: 0.5714 - val_loss: 1.4060\n",
      "Epoch 8/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 709ms/step - accuracy: 0.5359 - loss: 1.0671 - val_accuracy: 0.5714 - val_loss: 1.4031\n",
      "Epoch 9/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 640ms/step - accuracy: 0.5679 - loss: 0.9912 - val_accuracy: 0.5714 - val_loss: 1.4394\n",
      "Epoch 10/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 612ms/step - accuracy: 0.5819 - loss: 1.0098 - val_accuracy: 0.5714 - val_loss: 1.4354\n",
      "Found 38 images belonging to 4 classes.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 170ms/step - accuracy: 0.4545 - loss: 1.3139\n",
      "Best Emotion Recognition Model Test Accuracy: 0.45\n",
      "Best Emotion Recognition Model Test Loss: 1.32\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from keras_tuner import RandomSearch\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "import os\n",
    "# Verify number of classes\n",
    "emotion_num_classes = emotion_train_generator.num_classes\n",
    "print(f'Number of classes: {num_classes}')\n",
    "\n",
    "# Pre-trained model\n",
    "emotion_base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "# Fine-tune the model\n",
    "emotion_base_model.trainable = True\n",
    "fine_tune_at = 100\n",
    "for layer in emotion_base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Function to build the model (for hyperparameter tuning)\n",
    "def emotion_build_model(hp):\n",
    "    emotion_model = Sequential([\n",
    "        emotion_base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(hp.Int('units', min_value=128, max_value=512, step=64), activation='relu'),\n",
    "        Dropout(hp.Float('dropout', min_value=0.2, max_value=0.5, step=0.1)),\n",
    "        Dense(emotion_num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    emotion_model.compile(optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return emotion_model\n",
    "\n",
    "# Hyperparameter tuning\n",
    "tuner = RandomSearch(\n",
    "    emotion_build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=15,\n",
    "    executions_per_trial=2,\n",
    "    directory='my_dir',\n",
    "    project_name='Emotion_recognition'\n",
    ")\n",
    "\n",
    "# Perform the search\n",
    "tuner.search(emotion_train_generator, validation_data=emotion_validation_generator, epochs=10)\n",
    "\n",
    "# Get the best model\n",
    "emotion_best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Further train the best model\n",
    "emotion_best_model.fit(emotion_train_generator, validation_data=emotion_validation_generator, epochs=10)\n",
    "\n",
    "# Save the best model\n",
    "emotion_best_model.save('best_emotion_model.keras')\n",
    "\n",
    "# Evaluate the best model\n",
    "emotion_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "emotion_test_generator = emotion_test_datagen.flow_from_directory(emotion_validation_dir, target_size=(128, 128), batch_size=32, class_mode='categorical')\n",
    "\n",
    "test_loss, test_accuracy = emotion_best_model.evaluate(emotion_test_generator)\n",
    "print(f'Best Emotion Recognition Model Test Accuracy: {test_accuracy:.2f}')\n",
    "print(f'Best Emotion Recognition Model Test Loss: {test_loss:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e43aae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save('best_animal_recognition_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123941f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
